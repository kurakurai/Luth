base_model: "LiquidAI/LFM2-700M"

output_dir: "trained_model/"
deepspeed: configs/train/zero1.json # Multi-GPU training config

flash_attention: true
train_on_inputs: false
train_on_eos: 'turn'
sample_packing: true
chunked_cross_entropy: true
learning_rate: 2e-5
sequence_len: 16384  # larger sequence length improves packing efficiency for more tokens/sec
gradient_checkpointing: true # tradeoff reduced VRAM for increased time
optimizer: "adamw_torch_8bit"
lr_scheduler: "cosine"
warmup_ratio: 0.1
weight_decay: 0.01
bf16: true
fp16: false
tf32: false
max_grad_norm: 0.1
num_epochs: 3
save_strategy: "epoch"
logging_steps: 1

# lora_r: 16
# lora_alpha: 32
# lora_target_modules:
#   - "q_proj"
#   - "k_proj"
#   - "v_proj"
#   - "o_proj"

# Batch size per gpu = micro_batch_size * gradient_accumulation_steps
gradient_accumulation_steps: 4
micro_batch_size: 6
# eval_batch_size: 4

use_wandb: true
wandb_project: "Kurakura AI"
wandb_name: luth-lfm2-350m

chat_template: "tokenizer_default"
datasets:
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "luth_scholar"
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "luth_smoltalk2"
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "luth_croissantllm"
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "luth_aya_dataset"
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "luth_tulu3_persona_math"
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "luth_openhermes"
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "luth_tulu3_persona_instruct"


eot_tokens:
  - "<|im_end|>"
dataloader_prefetch_factor: 8
dataloader_num_workers: 2
dataloader_pin_memory: true

special_tokens: