base_model: "Qwen/Qwen3-0.6B"

output_dir: "trained_model/luth"
hub_model_id: "kurakurai/Luth-0.6B-fr"
deepspeed: configs/train/zero1.json # Multi-GPU training config

flash_attention: true
train_on_inputs: false
train_on_eos: 'turn'
sample_packing: true
chunked_cross_entropy: true
learning_rate: 2e-5
sequence_len: 16384  # larger sequence length improves packing efficiency for more tokens/sec
gradient_checkpointing: true # tradeoff reduced VRAM for increased time
optimizer: "adamw_torch_8bit"
lr_scheduler: "cosine"
warmup_ratio: 0.1
weight_decay: 0.01
bf16: true
fp16: false
tf32: false
max_grad_norm: 0.1
num_epochs: 3
save_strategy: "epoch"
logging_steps: 1

# lora_r: 16
# lora_alpha: 32
# lora_target_modules:
#   - "q_proj"
#   - "k_proj"
#   - "v_proj"
#   - "o_proj"

# Batch size per gpu = micro_batch_size * gradient_accumulation_steps
gradient_accumulation_steps: 4
micro_batch_size: 6
# eval_batch_size: 4

use_wandb: true
wandb_project: "Kurakura AI"
wandb_name: luth-run-qwen3-0.6b

chat_template: "tokenizer_default"
datasets:
  - path: "kurakurai/tulu-3-persona-math-fr"
    type: "chat_template"
    split: "train"
  - path: "kurakurai/tulu-3-persona-instruct-fr"
    type: "chat_template"
    split: "train"
  - path: "kurakurai/luth-sft"
    type: "chat_template"
    split: "train"
  - path: "kurakurai/Scholar-fr"
    type: "chat_template"
    split: "Scholar_all"
    field_messages: conversations
  - path: "kurakurai/openhermes-fr-filtered"
    type: "chat_template"
    split: "train"

dataloader_prefetch_factor: 8
dataloader_num_workers: 2
dataloader_pin_memory: true

special_tokens: